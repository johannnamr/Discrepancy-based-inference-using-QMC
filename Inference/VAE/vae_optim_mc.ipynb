{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vae_optim_mc.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMZids8quBqex3ZDDEkBYrx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johannnamr/Discrepancy-based-inference-using-QMC/blob/main/Inference/VAE/vae_optim_mc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt_cwPBPPEOy"
      },
      "source": [
        "# Variational autoencoder (MC)\n",
        "\n",
        "**Notebook for training a VAE using MC**\n",
        "\n",
        "Code is adjusted from https://github.com/audeg/Sinkhorn-GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggqrazKFP4rJ"
      },
      "source": [
        "## Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr7SWndFP6_g",
        "outputId": "9dc07c6f-8a10-41bc-828f-163e8a66b841"
      },
      "source": [
        "# mount my drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnrYr27imebu"
      },
      "source": [
        "Set path for saving the results (adjust if necessary):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jAap2wimiYe"
      },
      "source": [
        "path = '/content/drive/My Drive/Colab Notebooks/Paper/Inference/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Le5L7RFyGs1"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C00OrhMiR1Gd",
        "outputId": "37da2ca9-3273-45a7-cbfb-9de4726df1cb"
      },
      "source": [
        "! pip install tensorflow==1.15.0\n",
        "! pip install pytictoc"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==1.15.0\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 24 kB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.13.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 77.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.17.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.37.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.12.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 28.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.42.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.6.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.0) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=4f78d3588d5b5cb5026ea5d78e04e4d3f4e2e4ad983d40bd8f16e4fad52fd19f\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.6 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n",
            "Collecting pytictoc\n",
            "  Downloading pytictoc-1.5.2-py2.py3-none-any.whl (4.0 kB)\n",
            "Installing collected packages: pytictoc\n",
            "Successfully installed pytictoc-1.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWx3duJQWrMM"
      },
      "source": [
        "# This code is heavily based on Jan Mentzen's implementation of a VAE (https://jmetzen.github.io/2015-11-27/vae.html)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from pytictoc import TicToc # timer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jruG9VQWsv0"
      },
      "source": [
        "np.random.seed(0)\n",
        "tf.set_random_seed(0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwa0v1HHPu1o"
      },
      "source": [
        "## Load MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSLgdum9WuIq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c156c46-3ad0-4ac8-81d9-882340bf2ec6"
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
        "n_samples = mnist.train.num_examples"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-1eedde87f71e>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hAPGI1ytJy7"
      },
      "source": [
        "## Define useful functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XQN-2yVWwrB"
      },
      "source": [
        "# define some useful functions\n",
        "\n",
        "def init_xavier(n_in,n_out):\n",
        "    '''Create a convolution filter variable with the specified name and shape,\n",
        "    and initialize it using Xavier initialition.'''\n",
        "    initializer = tf.contrib.layers.xavier_initializer()\n",
        "    variable = tf.Variable(initializer(shape=[n_in,n_out]))\n",
        "    return variable\n",
        "\n",
        "def cost_mat(X,Y,N,M):\n",
        "    XX = tf.reduce_sum(tf.multiply(X,X),axis=1)\n",
        "    YY = tf.reduce_sum(tf.multiply(Y,Y),axis=1)\n",
        "    C1 = tf.transpose(tf.reshape(tf.tile(XX,[M]),[M,N]))\n",
        "    C2 = tf.reshape(tf.tile(YY,[N]),[N,M])\n",
        "    C3 = tf.transpose(tf.matmul(Y,tf.transpose(X)))\n",
        "    C = C1 + C2 - 2*C3;\n",
        "    return C\n",
        "\n",
        "def K_tild(u,v,C,N,M,epsilon):\n",
        "    C_tild = C - tf.transpose(tf.reshape(tf.tile(u[:,0],[M]),[M,N])) - tf.reshape(tf.tile(v[:,0],[N]),[N,M])\n",
        "    K_tild = tf.exp(-C_tild/epsilon)\n",
        "    return K_tild\n",
        "\n",
        "def sinkhorn_step_log(j,u,v,C, N,M,epsilon,Lambda = 1):\n",
        "    mu = tf.cast(1/N, tf.float32)\n",
        "    nu = tf.cast(1/M, tf.float32)\n",
        "    Ku = tf.reshape( tf.reduce_sum(K_tild(u,v,C,N,M,epsilon),axis = 1) ,[N,1] )\n",
        "    u = Lambda * ( epsilon*(tf.log(mu) - tf.log(Ku +10**(-6))) + u )\n",
        "    Kv = tf.reshape( tf.reduce_sum(K_tild(u,v,C,N,M,epsilon),axis = 0), [M,1] )\n",
        "    v = Lambda * ( epsilon*(tf.log(nu) - tf.log(Kv +10**(-6))) + v )\n",
        "    j += 1\n",
        "    return j,u,v,C,N,M,epsilon\n",
        "\n",
        "def sinkhorn_loss(X,Y):\n",
        "    epsilon = tf.constant(1.) # smoothing sinkhorn\n",
        "    Lambda = tf.constant(1.) # unbalanced parameter\n",
        "    k = tf.constant(50) # number of iterations for sinkhorn\n",
        "    N = tf.shape(X)[0] # sample size from mu_theta\n",
        "    M = tf.shape(Y)[0] # sample size from \\hat nu\n",
        "    D = tf.shape(Y)[1] # dimension of the obervation space\n",
        "    C = cost_mat(X,Y,N,M)\n",
        "    K = tf.exp(-C/epsilon)\n",
        "    #sinkhorn iterations\n",
        "    j0 = tf.constant(0)\n",
        "    u0 = tf.zeros([N,1])\n",
        "    v0 = tf.zeros([M,1])\n",
        "    cond_iter = lambda j, u, v, C, N, M, epsilon: j < k\n",
        "    j,u,v,C,N,M,epsilon = tf.while_loop(\n",
        "    cond_iter, sinkhorn_step_log, loop_vars=[j0, u0, v0,C, N,M,epsilon])\n",
        "    gamma_log = K_tild(u,v,C,N,M,epsilon)\n",
        "    final_cost = tf.reduce_sum(gamma_log*C)\n",
        "    return final_cost"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mEy3dhGthy3"
      },
      "source": [
        "## Define variational autoencoder class "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuidZwbkXxLI"
      },
      "source": [
        "# Variational Autoencoder class\n",
        "\n",
        "\n",
        "class VariationalAutoencoder(object):\n",
        "    \n",
        "    def __init__(self, network_architecture, transfer_fct=tf.nn.softplus, \n",
        "                 learning_rate=0.001, batch_size=100):\n",
        "        self.network_architecture = network_architecture\n",
        "        self.transfer_fct = transfer_fct\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        # tf Graph input\n",
        "        self.x = tf.placeholder(tf.float32, [batch_size, network_architecture[\"n_input\"]])\n",
        "      \n",
        "        # Create autoencoder network\n",
        "        self._create_network()\n",
        "        # Define loss function based variational upper-bound and \n",
        "        # corresponding optimizer\n",
        "        self._create_loss_optimizer()\n",
        "        \n",
        "        # Initializing the tensor flow variables\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        # Launch the session\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(init)\n",
        "    \n",
        "    def _create_network(self):\n",
        "        # Initialize autoencode network weights and biases\n",
        "        self.network_weights = self._initialize_weights(**self.network_architecture)\n",
        "\n",
        "        # Draw one sample z from uniform in latent space\n",
        "        n_z = self.network_architecture[\"n_z\"]\n",
        "        self.z = tf.random_uniform((self.batch_size, n_z), dtype=tf.float32)\n",
        "        \n",
        "        # Use generator to determine mean of\n",
        "        # Bernoulli distribution of reconstructed input\n",
        "        self.x_reconstr =   self._generator_network(self.network_weights[\"weights_gener\"],\n",
        "                                    self.network_weights[\"biases_gener\"])\n",
        "    \n",
        "    def _initialize_weights(self, n_hidden_gener_1,  n_hidden_gener_2, \n",
        "                            n_input, n_z):\n",
        "        all_weights = dict()\n",
        "        all_weights['weights_gener'] = {\n",
        "            'h1': init_xavier(n_z, n_hidden_gener_1),\n",
        "            'h2': init_xavier(n_hidden_gener_1, n_hidden_gener_2),\n",
        "            'out_var': init_xavier(n_hidden_gener_2, n_input)}\n",
        "        all_weights['biases_gener'] = {\n",
        "            'b1': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32)),\n",
        "            'b2': tf.Variable(tf.zeros([n_hidden_gener_2], dtype=tf.float32)),\n",
        "            'out_var': tf.Variable(tf.zeros([n_input], dtype=tf.float32))}\n",
        "        return all_weights        \n",
        "   \n",
        "\n",
        "    def _generator_network(self, weights, biases):\n",
        "        # Generate probabilistic decoder (decoder network), which\n",
        "        # maps points in latent space onto a Bernoulli distribution in data space.\n",
        "        # The transformation is parametrized and can be learned.\n",
        "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.z, weights['h1']), \n",
        "                                           biases['b1'])) \n",
        "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
        "                                           biases['b2'])) \n",
        "        x_reconstr = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['out_var']), \n",
        "                                 biases['out_var']))\n",
        "        return x_reconstr\n",
        "            \n",
        "    def _create_loss_optimizer(self):\n",
        "        # Sinkhorn loss\n",
        "        self.cost = sinkhorn_loss(self.x, self.x_reconstr)   # average over batch\n",
        "        # Use ADAM optimizer\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
        "        \n",
        "    def partial_fit(self, X):\n",
        "        \"\"\"Train model based on mini-batch of input data.\n",
        "        \n",
        "        Return cost of mini-batch.\n",
        "        \"\"\"\n",
        "        opt, cost = self.sess.run((self.optimizer, self.cost), \n",
        "                                  feed_dict={self.x: X})\n",
        "        return cost\n",
        "   \n",
        "    def generate(self, z_sample):\n",
        "        \"\"\" Generate data by sampling from latent space.\n",
        "        \n",
        "        If z_mu is not None, data for this point in latent space is\n",
        "        generated. Otherwise, z_mu is drawn from prior in latent \n",
        "        space.        \n",
        "        \"\"\"\n",
        "        \n",
        "        zz = tf.placeholder(tf.float32, [1, network_architecture[\"n_z\"]])\n",
        "\n",
        "        \n",
        "        weights = self.network_weights[\"weights_gener\"]\n",
        "        biases = self.network_weights[\"biases_gener\"]\n",
        "        \n",
        "        layer_1 = self.transfer_fct(tf.add(tf.matmul(zz, weights['h1']), \n",
        "                                           biases['b1'])) \n",
        "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
        "                                           biases['b2'])) \n",
        "        x_reconstr = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['out_var']), \n",
        "                                 biases['out_var']))\n",
        "        \n",
        "        return self.sess.run(x_reconstr,feed_dict={zz: np.reshape(z_sample,[1,network_architecture[\"n_z\"]])})"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMPYzDOUyYZF"
      },
      "source": [
        "## Define training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUbSqBPJxiKW"
      },
      "source": [
        "# Training\n",
        "\n",
        "def train(network_architecture, learning_rate=0.005,\n",
        "          batch_size=300, training_epochs=10, display_step=5):\n",
        "  \n",
        "    # create timer instance\n",
        "    t = TicToc()\n",
        "    t.tic()\n",
        "\n",
        "    print('Compiling...')\n",
        "    vae = VariationalAutoencoder(network_architecture, \n",
        "                                 learning_rate=learning_rate, \n",
        "                                 batch_size=batch_size)\n",
        "    # Training cycle\n",
        "    print('Training...')\n",
        "    loss = []\n",
        "    for epoch in range(training_epochs):\n",
        "        print(epoch)\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(n_samples / batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_xs, _ = mnist.train.next_batch(batch_size)\n",
        "\n",
        "            # Fit training using batch data\n",
        "            cost = vae.partial_fit(batch_xs)\n",
        "            # Compute average loss\n",
        "            avg_cost += cost / n_samples * batch_size\n",
        "\n",
        "        loss.append(avg_cost)\n",
        "\n",
        "        # Display logs per epoch step\n",
        "        if epoch % display_step == 0:\n",
        "            print(\"Epoch:\", '%04d' % (epoch+1), \n",
        "                  \"cost=\", \"{:.9f}\".format(avg_cost))\n",
        "            \n",
        "    t.toc()\n",
        "    return vae, loss"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w1sMar5ycG_"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5wGvuJcW4VY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e31e23cf-86e4-4177-bbd6-59ffa7de20d8"
      },
      "source": [
        "# training the model\n",
        "\n",
        "network_architecture =     dict(n_hidden_gener_1=500, # 1st layer decoder neurons\n",
        "         n_hidden_gener_2=500, # 2nd layer decoder neurons\n",
        "         n_input=784, # MNIST data input (img shape: 28*28)\n",
        "         n_z=2)  # dimensionality of latent space\n",
        "\n",
        "vae, loss = train(network_architecture, training_epochs=500)\n",
        "\n",
        "np.savez(path+\"vae_optim_mc.npz\",loss=loss)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling...\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Training...\n",
            "0\n",
            "Epoch: 0001 cost= 52.981856481\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "Epoch: 0006 cost= 39.263881219\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Epoch: 0011 cost= 35.873086978\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "Epoch: 0016 cost= 34.902169169\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "Epoch: 0021 cost= 34.408078017\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "Epoch: 0026 cost= 34.024189568\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "Epoch: 0031 cost= 33.750106867\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "Epoch: 0036 cost= 33.619202707\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "Epoch: 0041 cost= 33.482931706\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "Epoch: 0046 cost= 33.348920954\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "Epoch: 0051 cost= 33.273623033\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "Epoch: 0056 cost= 33.258318069\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "Epoch: 0061 cost= 33.150102081\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "Epoch: 0066 cost= 33.236484125\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "Epoch: 0071 cost= 33.150850015\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "Epoch: 0076 cost= 33.083481331\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "Epoch: 0081 cost= 32.920967456\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "Epoch: 0086 cost= 32.782901875\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "Epoch: 0091 cost= 32.718231062\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "Epoch: 0096 cost= 32.796496988\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "Epoch: 0101 cost= 32.624692268\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "Epoch: 0106 cost= 32.747422028\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "Epoch: 0111 cost= 32.634557856\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "Epoch: 0116 cost= 32.703232581\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "Epoch: 0121 cost= 32.468372976\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "Epoch: 0126 cost= 32.515309171\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "Epoch: 0131 cost= 32.417530278\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "Epoch: 0136 cost= 32.430441732\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "Epoch: 0141 cost= 32.393519232\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "Epoch: 0146 cost= 32.429029812\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "Epoch: 0151 cost= 32.256690695\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "Epoch: 0156 cost= 32.287476571\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "Epoch: 0161 cost= 32.150529463\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "Epoch: 0166 cost= 32.180583364\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "Epoch: 0171 cost= 32.124968792\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "Epoch: 0176 cost= 32.101332113\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "Epoch: 0181 cost= 32.042277853\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "Epoch: 0186 cost= 32.027351480\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "Epoch: 0191 cost= 31.942191166\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "Epoch: 0196 cost= 31.870039631\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "Epoch: 0201 cost= 31.816634965\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "Epoch: 0206 cost= 31.711893529\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "Epoch: 0211 cost= 31.739887536\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "Epoch: 0216 cost= 31.823905657\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "Epoch: 0221 cost= 31.723100270\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "Epoch: 0226 cost= 31.689102381\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "Epoch: 0231 cost= 31.683943228\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "Epoch: 0236 cost= 31.621102399\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "Epoch: 0241 cost= 31.482459661\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "Epoch: 0246 cost= 31.570551515\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "Epoch: 0251 cost= 31.403519252\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "Epoch: 0256 cost= 31.503832397\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "Epoch: 0261 cost= 31.525093470\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "Epoch: 0266 cost= 31.499920110\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "Epoch: 0271 cost= 31.328834849\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "Epoch: 0276 cost= 31.390484213\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "Epoch: 0281 cost= 31.214518277\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "Epoch: 0286 cost= 31.316721850\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "Epoch: 0291 cost= 31.247987761\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "Epoch: 0296 cost= 31.161852535\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "Epoch: 0301 cost= 31.295762183\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "Epoch: 0306 cost= 31.172600431\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "Epoch: 0311 cost= 31.193137769\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "Epoch: 0316 cost= 31.263063927\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "Epoch: 0321 cost= 31.116837637\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "Epoch: 0326 cost= 31.077596158\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "Epoch: 0331 cost= 31.164182614\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "Epoch: 0336 cost= 31.048962260\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "Epoch: 0341 cost= 31.051155957\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "Epoch: 0346 cost= 30.951551451\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "Epoch: 0351 cost= 31.027161796\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "Epoch: 0356 cost= 30.943181066\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "Epoch: 0361 cost= 30.888510590\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "Epoch: 0366 cost= 30.978476268\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "Epoch: 0371 cost= 30.998556914\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "Epoch: 0376 cost= 30.785730619\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "Epoch: 0381 cost= 30.942167514\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "Epoch: 0386 cost= 30.842863676\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "Epoch: 0391 cost= 30.818224432\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "Epoch: 0396 cost= 30.832459717\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "Epoch: 0401 cost= 30.833034065\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "Epoch: 0406 cost= 30.785901853\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "Epoch: 0411 cost= 30.879925190\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "Epoch: 0416 cost= 30.702883540\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "Epoch: 0421 cost= 30.723385433\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "Epoch: 0426 cost= 30.802545652\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "Epoch: 0431 cost= 30.685003416\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "Epoch: 0436 cost= 30.798348292\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "Epoch: 0441 cost= 30.764040018\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "Epoch: 0446 cost= 30.795296672\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "Epoch: 0451 cost= 30.627923823\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "Epoch: 0456 cost= 30.604876109\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "Epoch: 0461 cost= 30.608756873\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "Epoch: 0466 cost= 30.770112430\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "Epoch: 0471 cost= 30.591750620\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "Epoch: 0476 cost= 30.620903268\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "Epoch: 0481 cost= 30.532651853\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "Epoch: 0486 cost= 30.668218325\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "Epoch: 0491 cost= 30.637406294\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "Epoch: 0496 cost= 30.567804212\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "Elapsed time is 4003.558646 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToJEAT9-zEh0"
      },
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(loss)\n",
        "plt.set_ylim=(53,30)\n",
        "plt.savefig(\"loss.png\", bbox_inches=\"tight\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWgM4YrkW1Hp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56c960c1-78bf-408e-b0d8-b89296d901b2"
      },
      "source": [
        "# Visualizing manifold in 2D\n",
        "\n",
        "print('Generating images...')\n",
        "\n",
        "nx = ny = 15\n",
        "x_values = np.linspace(0, 1, nx)\n",
        "y_values = np.linspace(0, 1, ny)\n",
        "\n",
        "canvas = np.empty((28*ny, 28*nx))\n",
        "for i, yi in enumerate(x_values):\n",
        "    for j, xi in enumerate(y_values):\n",
        "        z_mu = np.array([xi, yi])\n",
        "        x_mean = vae.generate(z_mu)\n",
        "        canvas[(nx-i-1)*28:(nx-i)*28, j*28:(j+1)*28] = x_mean[0].reshape(28, 28)\n",
        "\n",
        "plt.figure(figsize=(8, 10))        \n",
        "Xi, Yi = np.meshgrid(x_values, y_values)\n",
        "plt.imshow(canvas, origin=\"upper\", cmap=\"gray\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"manifold.png\", bbox_inches=\"tight\")\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating images...\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}