{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "vae_sample_complexity.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN49P3A3fgR+my+BtTNr9Lt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johannnamr/Discrepancy-based-inference-using-QMC/blob/main/Inference/VAE/vae_sample_complexity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cplPVklkvTHX"
      },
      "source": [
        "# Sample complexity for a VAE using MC and RQMC\n",
        "\n",
        "Notebook calculating the sample complexity for a VAE using MC and RQMC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDG0HORGvoZk"
      },
      "source": [
        "## Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6VUXJc7vu9c",
        "outputId": "9e5c508d-5bb7-473c-c4e0-116a77d048f8"
      },
      "source": [
        "# mount my drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Finzw3zaaZ8o"
      },
      "source": [
        "%run \"/content/drive/My Drive/Colab Notebooks/ot_slicedW.ipynb\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnrYr27imebu"
      },
      "source": [
        "Set path for saving the results (adjust if necessary):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jAap2wimiYe"
      },
      "source": [
        "path = '/content/drive/My Drive/Colab Notebooks/Paper/Inference/'\n",
        "path_samples = '/content/drive/My Drive/Colab Notebooks/Paper/Inference/VAE_samples/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TxiAHFXvmTC"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwR_kUqzvLuT",
        "outputId": "8267c94d-26ac-44fd-c8c1-74ca85bac1bb"
      },
      "source": [
        "! pip install tensorflow==1.15.0\n",
        "! pip install --upgrade scipy # update scipy to latest version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==1.15.0\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 23 kB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.37.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.42.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 70.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.13.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 48.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.6.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.0) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=3b2bd0540eecbad544251e5c2b9d6635ffb6b9470a7181cf6382058ecd7de8d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.6 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.19.5)\n",
            "Installing collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.6 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed scipy-1.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EWZWCs0v78x"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from scipy.stats import qmc # QMC points\n",
        "import scipy.spatial.distance as distance # distance used for kernel\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XfT1HfUjiP0"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ARTAhZdph7r"
      },
      "source": [
        "Set parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luwU-LOpjkfT"
      },
      "source": [
        "n = [ 2**j for j in range(4,15) ]         # sample size  \n",
        "num = 25                                  # numbers of repetitions for MC and RQMC\n",
        "divergence = 'mmd'                         # sink: Sinkhorn, w: Wasserstein, sw: sliced Wasserstein, mmd = squared MMD\n",
        "runsim = False                             # True: simulate samples, False: load saved samples\n",
        "rundiv = True                             # True: calculate divergences, False: load saved divergences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbBawLFzjPS2"
      },
      "source": [
        "if rundiv and divergence == 'w' or rundiv and divergence == 'sw' or rundiv and divergence == 'sink':\n",
        "  #! pip install --upgrade pip # update pip to latest version\n",
        "  ! pip install --upgrade numpy # update numpy to latest version\n",
        "  ! pip install POT --quiet\n",
        "  import ot # Wasserstein distance and Sinkhorn divergence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hAPGI1ytJy7"
      },
      "source": [
        "## Define useful functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOpQmQC_c8KM"
      },
      "source": [
        "# Xavier initialisation\n",
        "\n",
        "def init_xavier(n_in,n_out):\n",
        "    '''Create a convolution filter variable with the specified name and shape,\n",
        "    and initialize it using Xavier initialition.'''\n",
        "    initializer = tf.contrib.layers.xavier_initializer()\n",
        "    variable = tf.Variable(initializer(shape=[n_in,n_out]))\n",
        "    return variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XQN-2yVWwrB"
      },
      "source": [
        "# Sinkhorn divergence\n",
        "\n",
        "def cost_mat(X,Y,N,M):\n",
        "    XX = tf.reduce_sum(tf.multiply(X,X),axis=1)\n",
        "    YY = tf.reduce_sum(tf.multiply(Y,Y),axis=1)\n",
        "    C1 = tf.transpose(tf.reshape(tf.tile(XX,[M]),[M,N]))\n",
        "    C2 = tf.reshape(tf.tile(YY,[N]),[N,M])\n",
        "    C3 = tf.transpose(tf.matmul(Y,tf.transpose(X)))\n",
        "    C = C1 + C2 - 2*C3;\n",
        "    return C\n",
        "\n",
        "def K_tild(u,v,C,N,M,epsilon):\n",
        "    C_tild = C - tf.transpose(tf.reshape(tf.tile(u[:,0],[M]),[M,N])) - tf.reshape(tf.tile(v[:,0],[N]),[N,M])\n",
        "    K_tild = tf.exp(-C_tild/epsilon)\n",
        "    return K_tild\n",
        "\n",
        "def sinkhorn_step_log(j,u,v,C, N,M,epsilon,Lambda = 1):\n",
        "    mu = tf.cast(1/N, tf.float32)\n",
        "    nu = tf.cast(1/M, tf.float32)\n",
        "    Ku = tf.reshape( tf.reduce_sum(K_tild(u,v,C,N,M,epsilon),axis = 1) ,[N,1] )\n",
        "    u = Lambda * ( epsilon*(tf.log(mu) - tf.log(Ku +10**(-6))) + u )\n",
        "    Kv = tf.reshape( tf.reduce_sum(K_tild(u,v,C,N,M,epsilon),axis = 0), [M,1] )\n",
        "    v = Lambda * ( epsilon*(tf.log(nu) - tf.log(Kv +10**(-6))) + v )\n",
        "    j += 1\n",
        "    return j,u,v,C,N,M,epsilon\n",
        "\n",
        "def sinkhorn_loss(X,Y):\n",
        "    epsilon = tf.constant(1.) # smoothing sinkhorn\n",
        "    Lambda = tf.constant(1.) # unbalanced parameter\n",
        "    k = tf.constant(50) # number of iterations for sinkhorn\n",
        "    N = tf.shape(X)[0] # sample size from mu_theta\n",
        "    M = tf.shape(Y)[0] # sample size from \\hat nu\n",
        "    D = tf.shape(Y)[1] # dimension of the obervation space\n",
        "    C = cost_mat(X,Y,N,M)\n",
        "    K = tf.exp(-C/epsilon)\n",
        "    #sinkhorn iterations\n",
        "    j0 = tf.constant(0)\n",
        "    u0 = tf.zeros([N,1])\n",
        "    v0 = tf.zeros([M,1])\n",
        "    cond_iter = lambda j, u, v, C, N, M, epsilon: j < k\n",
        "    j,u,v,C,N,M,epsilon = tf.while_loop(\n",
        "    cond_iter, sinkhorn_step_log, loop_vars=[j0, u0, v0,C, N,M,epsilon])\n",
        "    gamma_log = K_tild(u,v,C,N,M,epsilon)\n",
        "    final_cost = tf.reduce_sum(gamma_log*C)\n",
        "    return final_cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwDfvPT1EWMf"
      },
      "source": [
        "## Define divergences for sample complexity experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaFFeqpUrfKS"
      },
      "source": [
        "# Wasserstein and sliced Wasserstein distance\n",
        "\n",
        "def wasserstein_loss(X,Y):\n",
        "\n",
        "    n = np.shape(X)[0]\n",
        "\n",
        "    # equal weights\n",
        "    a = np.ones((n,)) / n \n",
        "    b = np.ones((n,)) / n\n",
        "    \n",
        "    # MC and RQMC\n",
        "    for r in range(num):\n",
        "      M = ot.dist(X, Y, 'euclidean')\n",
        "      M /= M.max()\n",
        "      w = ot.emd2(a, b, M)\n",
        "\n",
        "    return w\n",
        "\n",
        "def sliced_wasserstein_loss(X,Y):\n",
        "\n",
        "    n = np.shape(X)[0]\n",
        "\n",
        "    # equal weights\n",
        "    a = np.ones((n,)) / n \n",
        "    b = np.ones((n,)) / n\n",
        "    \n",
        "    sw = sliced_wasserstein_distance(X,Y, metric='euclidean', a=a, b=b, n_projections=100)\n",
        "\n",
        "    return sw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJj7RFjd1SDI"
      },
      "source": [
        "# Sinkhorn divergence using the OT library\n",
        "\n",
        "def sink_loss(X,Y):\n",
        "\n",
        "  n = np.shape(X)[0]\n",
        "\n",
        "  # equal weights\n",
        "  a = np.ones((n,)) / n \n",
        "  b = np.ones((n,)) / n\n",
        "\n",
        "  sink = ot.bregman.empirical_sinkhorn_divergence(X, Y, reg=1, a=a, b=b, metric='sqeuclidean',method='sinkhorn')\n",
        "\n",
        "  return sink"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g7kbEI4c5QN"
      },
      "source": [
        "# Gaussian kernel\n",
        "def k(X,Y,l): \n",
        "\n",
        "    r = distance.cdist(X,Y,'sqeuclidean')\n",
        "    K = np.exp(-(1/(2*l**2))*r)\n",
        "\n",
        "    return K\n",
        "\n",
        "# MMD^2\n",
        "def mmd_loss(n,m,kxx,kxy,kyy):\n",
        "\n",
        "    # first sum\n",
        "    sum1 = np.sum(kxx)\n",
        "    \n",
        "    # second sum\n",
        "    sum2 = np.sum(kxy)\n",
        "    \n",
        "    # third sum\n",
        "    sum3 = np.sum(kyy)\n",
        "    \n",
        "    return (1/n**2)*sum1-(2/(m*n))*sum2+(1/m**2)*sum3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mEy3dhGthy3"
      },
      "source": [
        "## Define variational autoencoder class "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuidZwbkXxLI"
      },
      "source": [
        "# Variational Autoencoder class\n",
        "\n",
        "\n",
        "class VariationalAutoencoder(object):\n",
        "    \n",
        "    def __init__(self, network_architecture, transfer_fct=tf.nn.softplus, \n",
        "                 learning_rate=0.001, batch_size=100):\n",
        "        self.network_architecture = network_architecture\n",
        "        self.transfer_fct = transfer_fct\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        # tf Graph input\n",
        "        self.x = tf.placeholder(tf.float32, [batch_size, network_architecture[\"n_input\"]])\n",
        "      \n",
        "        # Create autoencoder network\n",
        "        self._create_network()\n",
        "        # Define loss function based variational upper-bound and \n",
        "        # corresponding optimizer\n",
        "        self._create_loss_optimizer()\n",
        "        \n",
        "        # Initializing the tensor flow variables\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        # Launch the session\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(init)\n",
        "    \n",
        "    def _create_network(self):\n",
        "        # Initialize autoencode network weights and biases\n",
        "        self.network_weights = self._initialize_weights(**self.network_architecture)\n",
        "\n",
        "        # Draw one sample z from uniform in latent space\n",
        "        n_z = self.network_architecture[\"n_z\"]\n",
        "        self.z = tf.random_uniform((self.batch_size, n_z), dtype=tf.float32)\n",
        "        \n",
        "        # Use generator to determine mean of\n",
        "        # Bernoulli distribution of reconstructed input\n",
        "        self.x_reconstr =   self._generator_network(self.network_weights[\"weights_gener\"],\n",
        "                                    self.network_weights[\"biases_gener\"])\n",
        "    \n",
        "    def _initialize_weights(self, n_hidden_gener_1,  n_hidden_gener_2, \n",
        "                            n_input, n_z):\n",
        "        all_weights = dict()\n",
        "        all_weights['weights_gener'] = {\n",
        "            'h1': init_xavier(n_z, n_hidden_gener_1),\n",
        "            'h2': init_xavier(n_hidden_gener_1, n_hidden_gener_2),\n",
        "            'out_var': init_xavier(n_hidden_gener_2, n_input)}\n",
        "        all_weights['biases_gener'] = {\n",
        "            'b1': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32)),\n",
        "            'b2': tf.Variable(tf.zeros([n_hidden_gener_2], dtype=tf.float32)),\n",
        "            'out_var': tf.Variable(tf.zeros([n_input], dtype=tf.float32))}\n",
        "        return all_weights        \n",
        "   \n",
        "\n",
        "    def _generator_network(self, weights, biases):\n",
        "        # Generate probabilistic decoder (decoder network), which\n",
        "        # maps points in latent space onto a Bernoulli distribution in data space.\n",
        "        # The transformation is parametrized and can be learned.\n",
        "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.z, weights['h1']), \n",
        "                                           biases['b1'])) \n",
        "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
        "                                           biases['b2'])) \n",
        "        x_reconstr = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['out_var']), \n",
        "                                 biases['out_var']))\n",
        "        return x_reconstr\n",
        "            \n",
        "    def _create_loss_optimizer(self):\n",
        "        # Sinkhorn loss\n",
        "        self.cost = sinkhorn_loss(self.x, self.x_reconstr)   # average over batch\n",
        "        # Use ADAM optimizer\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
        "        \n",
        "    def partial_fit(self, X):\n",
        "        \"\"\"Train model based on mini-batch of input data.\n",
        "        \n",
        "        Return cost of mini-batch.\n",
        "        \"\"\"\n",
        "        opt, cost = self.sess.run((self.optimizer, self.cost), \n",
        "                                  feed_dict={self.x: X})\n",
        "        return cost    \n",
        "   \n",
        "    def generate(self, n, z_sample):\n",
        "        \"\"\" Generate data by sampling from latent space.\n",
        "        \n",
        "        If z_mu is not None, data for this point in latent space is\n",
        "        generated. Otherwise, z_mu is drawn from prior in latent \n",
        "        space.        \n",
        "        \"\"\"\n",
        "        \n",
        "        zz = tf.placeholder(tf.float32, [n, network_architecture[\"n_z\"]])\n",
        "\n",
        "        \n",
        "        weights = self.network_weights[\"weights_gener\"]\n",
        "        biases = self.network_weights[\"biases_gener\"]\n",
        "        \n",
        "        layer_1 = self.transfer_fct(tf.add(tf.matmul(zz, weights['h1']), \n",
        "                                           biases['b1'])) \n",
        "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
        "                                           biases['b2'])) \n",
        "        x_reconstr = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['out_var']), \n",
        "                                 biases['out_var']))\n",
        "        \n",
        "        return self.sess.run(x_reconstr,feed_dict={zz: np.reshape(z_sample,[n,network_architecture[\"n_z\"]])})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMPYzDOUyYZF"
      },
      "source": [
        "## Define training function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w1sMar5ycG_"
      },
      "source": [
        "## Initialise the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5wGvuJcW4VY"
      },
      "source": [
        "# initialise the model\n",
        "if runsim:\n",
        "  network_architecture =     dict(n_hidden_gener_1=500, # 1st layer decoder neurons\n",
        "         n_hidden_gener_2=500, # 2nd layer decoder neurons\n",
        "         n_input=784, # MNIST data input (img shape: 28*28)\n",
        "         n_z=2)  # dimensionality of latent space\n",
        "\n",
        "  vae = VariationalAutoencoder(network_architecture, learning_rate=0.005, batch_size=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL69j0wrXvbS"
      },
      "source": [
        "## Generate samples\n",
        "\n",
        "Use `vae.generate()` with either MC or QMC samples as input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBjLhJ8RK_VF"
      },
      "source": [
        "Sampling using RQMC:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUKqygC9gSkc"
      },
      "source": [
        "# rqmc\n",
        "if runsim:\n",
        "  for rep in range(num):\n",
        "    if rep % 5 == 0:\n",
        "      print(rep)\n",
        "    x_rqmc = np.zeros((np.max(n),784),dtype=np.float32)\n",
        "    y_rqmc = np.zeros((np.max(n),784),dtype=np.float32)\n",
        "    # x values\n",
        "    sampler_x = qmc.Sobol(d=2, scramble=True)\n",
        "    zx_rqmc = sampler_x.random(np.max(n))                \n",
        "    x_rqmc[:,:] = vae.generate(np.max(n),zx_rqmc)\n",
        "    np.savez(path_samples+\"vae_xsamples_rqmc_%s.npz\"%(rep),x_rqmc=x_rqmc)\n",
        "    # y values\n",
        "    sampler_y = qmc.Sobol(d=2, scramble=True)\n",
        "    zy_rqmc = sampler_y.random(np.max(n))               \n",
        "    y_rqmc[:,:] = vae.generate(np.max(n),zy_rqmc)\n",
        "    np.savez(path_samples+\"vae_ysamples_rqmc_%s.npz\" %(rep),y_rqmc=y_rqmc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJw0FB5_h-1v"
      },
      "source": [
        "if runsim==False:\n",
        "  x_rqmc = np.zeros((num,np.max(n),784),dtype=np.float32)\n",
        "  y_rqmc = np.zeros((num,np.max(n),784),dtype=np.float32)\n",
        "  for rep in range(num):\n",
        "    xsamples_rqmc = np.load(path_samples+\"vae_xsamples_rqmc_%s.npz\"%(rep))\n",
        "    ysamples_rqmc = np.load(path_samples+\"vae_ysamples_rqmc_%s.npz\"%(rep))\n",
        "    x_rqmc[rep,:,:] = xsamples_rqmc['x_rqmc']\n",
        "    y_rqmc[rep,:,:] = ysamples_rqmc['y_rqmc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SDBHdmlLCTc"
      },
      "source": [
        "Sampling using MC:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5tklxlxhUWs"
      },
      "source": [
        "# mc\n",
        "if runsim:\n",
        "  for rep in range(num):\n",
        "    if rep % 5 == 0:\n",
        "      print(rep)\n",
        "    x_mc = np.zeros((np.max(n),784),dtype=np.float32)\n",
        "    y_mc = np.zeros((np.max(n),784),dtype=np.float32)\n",
        "    # x values\n",
        "    zx_mc = np.random.rand(np.max(n),2)           \n",
        "    x_mc[:,:] = vae.generate(np.max(n),zx_mc)\n",
        "    np.savez(path_samples+\"vae_xsamples_mc_%s.npz\" %(rep),x_mc=x_mc)\n",
        "    # y values\n",
        "    zy_mc = np.random.rand(np.max(n),2)            \n",
        "    y_mc[:,:] = vae.generate(np.max(n),zy_mc)\n",
        "    np.savez(path_samples+\"vae_ysamples_mc_%s.npz\"%(rep),y_mc=y_mc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK122aCSHSWh"
      },
      "source": [
        "if runsim==False:\n",
        "  x_mc = np.zeros((num,np.max(n),784),dtype=np.float32)\n",
        "  y_mc = np.zeros((num,np.max(n),784),dtype=np.float32)\n",
        "  for rep in range(num):\n",
        "    xsamples_mc = np.load(path_samples+\"vae_xsamples_mc_%s.npz\"%(rep))\n",
        "    ysamples_mc = np.load(path_samples+\"vae_ysamples_mc_%s.npz\"%(rep))\n",
        "    x_mc[rep,:,:] = xsamples_mc['x_mc']\n",
        "    y_mc[rep,:,:] = ysamples_mc['y_mc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoLzuud9gNtz"
      },
      "source": [
        "## Compute divergence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd8d1w2ygP1Q",
        "outputId": "22e96e53-0b8a-4777-85d1-fba6c4d4a174"
      },
      "source": [
        "# RQMC\n",
        "\n",
        "if rundiv:\n",
        "  loss_rqmc = np.zeros((len(n),num))\n",
        "  for rep in range(num):\n",
        "    if rep % 5 == 0:\n",
        "      print(rep)      \n",
        "    for i in range(len(n)):\n",
        "      # divergence\n",
        "      if divergence == 'sink':\n",
        "        loss_rqmc[i,rep] = sink_loss(x_rqmc[rep,:n[i],:], y_rqmc[rep,:n[i],:])\n",
        "      if divergence == 'w':\n",
        "        loss_rqmc[i,rep] = wasserstein_loss(x_rqmc[rep,:n[i],:], y_rqmc[rep,:n[i],:])\n",
        "      if divergence == 'sw':\n",
        "        loss_rqmc[i,rep] = sliced_wasserstein_loss(x_rqmc[rep,:n[i],:], y_rqmc[rep,:n[i],:])\n",
        "      if divergence == 'mmd':\n",
        "        loss_rqmc[i,rep] = mmd_loss(n[i],n[i],k(x_rqmc[rep,:n[i],:],x_rqmc[rep,:n[i],:],l=0.01),k(x_rqmc[rep,:n[i],:],y_rqmc[rep,:n[i],:],l=0.01),k(y_rqmc[rep,:n[i],:],y_rqmc[rep,:n[i],:],l=0.01))\n",
        "\n",
        "  print(\"RQMC:\")    \n",
        "  print(np.mean(np.abs(loss_rqmc),axis=1))\n",
        "  np.savez(path+\"vae_loss_%s_rqmc.npz\" %(divergence),loss=np.abs(loss_rqmc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "RQMC:\n",
            "[5.37293655e-02 1.48359990e-02 3.27543503e-03 6.64881582e-04\n",
            " 1.11745062e-04 1.68251383e-05 2.81478487e-06 3.57861618e-07\n",
            " 6.33573016e-08 7.21522113e-09 1.04696720e-09]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqiieIt7inN3",
        "outputId": "2fec9cd3-b39f-4568-ae22-fd05c9a6d7be"
      },
      "source": [
        "# MC\n",
        "\n",
        "if rundiv:\n",
        "  loss_mc = np.zeros((len(n),num))\n",
        "  for rep in range(num):\n",
        "    if rep % 5 == 0:\n",
        "      print(rep)      \n",
        "    for i in range(len(n)):\n",
        "      # divergence\n",
        "      if divergence == 'sink':\n",
        "        loss_mc[i,rep] = sink_loss(x_mc[rep,:n[i],:], y_mc[rep,:n[i],:])\n",
        "      if divergence == 'w':\n",
        "        loss_mc[i,rep] = wasserstein_loss(x_mc[rep,:n[i],:], y_mc[rep,:n[i],:])\n",
        "      if divergence == 'sw':\n",
        "        loss_mc[i,rep] = sliced_wasserstein_loss(x_mc[rep,:n[i],:], y_mc[rep,:n[i],:])\n",
        "      if divergence == 'mmd':\n",
        "        loss_mc[i,rep] = mmd_loss(n[i],n[i],k(x_mc[rep,:n[i],:],x_mc[rep,:n[i],:],l=0.01),k(x_mc[rep,:n[i],:],y_mc[rep,:n[i],:],l=0.01),k(y_mc[rep,:n[i],:],y_mc[rep,:n[i],:],l=0.01))\n",
        "\n",
        "  print(\"MC:\")    \n",
        "  print(np.mean(np.abs(loss_mc),axis=1))\n",
        "  np.savez(path+\"vae_loss_%s_mc.npz\" %(divergence),loss=np.abs(loss_mc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "MC:\n",
            "[1.14176596e-01 5.58370644e-02 2.82380863e-02 1.43520218e-02\n",
            " 7.15324455e-03 3.60997196e-03 1.69189756e-03 9.14639159e-04\n",
            " 4.61103698e-04 2.10666427e-04 1.12191447e-04]\n"
          ]
        }
      ]
    }
  ]
}