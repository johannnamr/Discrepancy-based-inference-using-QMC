{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ot_sink.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOIgeTn+bPVRCiPCMRX3Ig",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johannnamr/Discrepancy-based-inference-using-QMC/blob/main/Helper-functions/ot_sink.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Source code from the OT library for Sinkhorn divergence"
      ],
      "metadata": {
        "id": "sMa2JQ6yrN8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjusted for permanent use of the scipy backend to calculate pair-wise distances"
      ],
      "metadata": {
        "id": "oRxJcMgosljW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pythonot.github.io/_modules/ot/bregman.html#empirical_sinkhorn_divergence"
      ],
      "metadata": {
        "id": "dg6cxeaLrUJp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rH19iOJrGAA"
      },
      "outputs": [],
      "source": [
        "def empirical_sinkhorn_divergence_adj(X_s, X_t, reg, a=None, b=None, metric='sqeuclidean',\n",
        "                                  numIterMax=10000, stopThr=1e-9,\n",
        "                                  verbose=False, log=False, warn=True, **kwargs):\n",
        "    r'''\n",
        "    Compute the sinkhorn divergence loss from empirical data\n",
        "\n",
        "    The function solves the following optimization problems and return the\n",
        "    sinkhorn divergence :math:`S`:\n",
        "\n",
        "    .. math::\n",
        "\n",
        "        W &= \\min_\\gamma \\quad \\langle \\gamma, \\mathbf{M} \\rangle_F +\n",
        "        \\mathrm{reg} \\cdot\\Omega(\\gamma)\n",
        "\n",
        "        W_a &= \\min_{\\gamma_a} \\quad \\langle \\gamma_a, \\mathbf{M_a} \\rangle_F +\n",
        "        \\mathrm{reg} \\cdot\\Omega(\\gamma_a)\n",
        "\n",
        "        W_b &= \\min_{\\gamma_b} \\quad \\langle \\gamma_b, \\mathbf{M_b} \\rangle_F +\n",
        "        \\mathrm{reg} \\cdot\\Omega(\\gamma_b)\n",
        "\n",
        "        S &= W - \\frac{W_a + W_b}{2}\n",
        "\n",
        "    .. math::\n",
        "        s.t. \\ \\gamma \\mathbf{1} &= \\mathbf{a}\n",
        "\n",
        "             \\gamma^T \\mathbf{1} &= \\mathbf{b}\n",
        "\n",
        "             \\gamma &\\geq 0\n",
        "\n",
        "             \\gamma_a \\mathbf{1} &= \\mathbf{a}\n",
        "\n",
        "             \\gamma_a^T \\mathbf{1} &= \\mathbf{a}\n",
        "\n",
        "             \\gamma_a &\\geq 0\n",
        "\n",
        "             \\gamma_b \\mathbf{1} &= \\mathbf{b}\n",
        "\n",
        "             \\gamma_b^T \\mathbf{1} &= \\mathbf{b}\n",
        "\n",
        "             \\gamma_b &\\geq 0\n",
        "    where :\n",
        "\n",
        "    - :math:`\\mathbf{M}` (resp. :math:`\\mathbf{M_a}`, :math:`\\mathbf{M_b}`)\n",
        "      is the (`n_samples_a`, `n_samples_b`) metric cost matrix\n",
        "      (resp (`n_samples_a, n_samples_a`) and (`n_samples_b`, `n_samples_b`))\n",
        "    - :math:`\\Omega` is the entropic regularization term\n",
        "      :math:`\\Omega(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})`\n",
        "    - :math:`\\mathbf{a}` and :math:`\\mathbf{b}` are source and target weights (sum to 1)\n",
        "\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_s : array-like, shape (n_samples_a, dim)\n",
        "        samples in the source domain\n",
        "    X_t : array-like, shape (n_samples_b, dim)\n",
        "        samples in the target domain\n",
        "    reg : float\n",
        "        Regularization term >0\n",
        "    a : array-like, shape (n_samples_a,)\n",
        "        samples weights in the source domain\n",
        "    b : array-like, shape (n_samples_b,)\n",
        "        samples weights in the target domain\n",
        "    numItermax : int, optional\n",
        "        Max number of iterations\n",
        "    stopThr : float, optional\n",
        "        Stop threshold on error (>0)\n",
        "    verbose : bool, optional\n",
        "        Print information along iterations\n",
        "    log : bool, optional\n",
        "        record log if True\n",
        "    warn : bool, optional\n",
        "        if True, raises a warning if the algorithm doesn't convergence.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    W : (1,) array-like\n",
        "        Optimal transportation symmetrized loss for the given parameters\n",
        "    log : dict\n",
        "        log dictionary return only if log==True in parameters\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> n_samples_a = 2\n",
        "    >>> n_samples_b = 4\n",
        "    >>> reg = 0.1\n",
        "    >>> X_s = np.reshape(np.arange(n_samples_a, dtype=np.float64), (n_samples_a, 1))\n",
        "    >>> X_t = np.reshape(np.arange(0, n_samples_b, dtype=np.float64), (n_samples_b, 1))\n",
        "    >>> empirical_sinkhorn_divergence(X_s, X_t, reg)  # doctest: +ELLIPSIS\n",
        "    1.499887176049052\n",
        "\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [23] Aude Genevay, Gabriel Peyré, Marco Cuturi, Learning Generative\n",
        "        Models with Sinkhorn Divergences,  Proceedings of the Twenty-First\n",
        "        International Conference on Artficial Intelligence and Statistics,\n",
        "        (AISTATS) 21, 2018\n",
        "    '''\n",
        "    if log:\n",
        "        sinkhorn_loss_ab, log_ab = empirical_sinkhorn2_adj(X_s, X_t, reg, a, b, metric=metric,\n",
        "                                                       numIterMax=numIterMax,\n",
        "                                                       stopThr=1e-9, verbose=verbose,\n",
        "                                                       log=log, warn=warn, **kwargs)\n",
        "\n",
        "        sinkhorn_loss_a, log_a = empirical_sinkhorn2_adj(X_s, X_s, reg, a, a, metric=metric,\n",
        "                                                     numIterMax=numIterMax,\n",
        "                                                     stopThr=1e-9, verbose=verbose,\n",
        "                                                     log=log, warn=warn, **kwargs)\n",
        "\n",
        "        sinkhorn_loss_b, log_b = empirical_sinkhorn2_adj(X_t, X_t, reg, b, b, metric=metric,\n",
        "                                                     numIterMax=numIterMax,\n",
        "                                                     stopThr=1e-9, verbose=verbose,\n",
        "                                                     log=log, warn=warn, **kwargs)\n",
        "\n",
        "        sinkhorn_div = sinkhorn_loss_ab - 0.5 * (sinkhorn_loss_a + sinkhorn_loss_b)\n",
        "\n",
        "        log = {}\n",
        "        log['sinkhorn_loss_ab'] = sinkhorn_loss_ab\n",
        "        log['sinkhorn_loss_a'] = sinkhorn_loss_a\n",
        "        log['sinkhorn_loss_b'] = sinkhorn_loss_b\n",
        "        log['log_sinkhorn_ab'] = log_ab\n",
        "        log['log_sinkhorn_a'] = log_a\n",
        "        log['log_sinkhorn_b'] = log_b\n",
        "\n",
        "        return max(0, sinkhorn_div), log\n",
        "\n",
        "    else:\n",
        "        sinkhorn_loss_ab = empirical_sinkhorn2_adj(X_s, X_t, reg, a, b, metric=metric,\n",
        "                                               numIterMax=numIterMax, stopThr=1e-9,\n",
        "                                               verbose=verbose, log=log,\n",
        "                                               warn=warn, **kwargs)\n",
        "\n",
        "        sinkhorn_loss_a = empirical_sinkhorn2_adj(X_s, X_s, reg, a, a, metric=metric,\n",
        "                                              numIterMax=numIterMax, stopThr=1e-9,\n",
        "                                              verbose=verbose, log=log,\n",
        "                                              warn=warn, **kwargs)\n",
        "\n",
        "        sinkhorn_loss_b = empirical_sinkhorn2_adj(X_t, X_t, reg, b, b, metric=metric,\n",
        "                                              numIterMax=numIterMax, stopThr=1e-9,\n",
        "                                              verbose=verbose, log=log,\n",
        "                                              warn=warn, **kwargs)\n",
        "\n",
        "        sinkhorn_div = sinkhorn_loss_ab - 0.5 * (sinkhorn_loss_a + sinkhorn_loss_b)\n",
        "        return max(0, sinkhorn_div)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def empirical_sinkhorn2_adj(X_s, X_t, reg, a=None, b=None, metric='sqeuclidean',\n",
        "                        numIterMax=10000, stopThr=1e-9, isLazy=False,\n",
        "                        batchSize=100, verbose=False, log=False, warn=True, **kwargs):\n",
        "    r'''\n",
        "    Solve the entropic regularization optimal transport problem from empirical\n",
        "    data and return the OT loss\n",
        "\n",
        "\n",
        "    The function solves the following optimization problem:\n",
        "\n",
        "    .. math::\n",
        "        W = \\min_\\gamma \\quad \\langle \\gamma, \\mathbf{M} \\rangle_F +\n",
        "        \\mathrm{reg} \\cdot\\Omega(\\gamma)\n",
        "\n",
        "        s.t. \\ \\gamma \\mathbf{1} &= \\mathbf{a}\n",
        "\n",
        "             \\gamma^T \\mathbf{1} &= \\mathbf{b}\n",
        "\n",
        "             \\gamma &\\geq 0\n",
        "    where :\n",
        "\n",
        "    - :math:`\\mathbf{M}` is the (`n_samples_a`, `n_samples_b`) metric cost matrix\n",
        "    - :math:`\\Omega` is the entropic regularization term\n",
        "      :math:`\\Omega(\\gamma)=\\sum_{i,j} \\gamma_{i,j}\\log(\\gamma_{i,j})`\n",
        "    - :math:`\\mathbf{a}` and :math:`\\mathbf{b}` are source and target weights (sum to 1)\n",
        "\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_s : array-like, shape (n_samples_a, dim)\n",
        "        samples in the source domain\n",
        "    X_t : array-like, shape (n_samples_b, dim)\n",
        "        samples in the target domain\n",
        "    reg : float\n",
        "        Regularization term >0\n",
        "    a : array-like, shape (n_samples_a,)\n",
        "        samples weights in the source domain\n",
        "    b : array-like, shape (n_samples_b,)\n",
        "        samples weights in the target domain\n",
        "    numItermax : int, optional\n",
        "        Max number of iterations\n",
        "    stopThr : float, optional\n",
        "        Stop threshold on error (>0)\n",
        "    isLazy: boolean, optional\n",
        "        If True, then only calculate the cost matrix by block and return\n",
        "        the dual potentials only (to save memory). If False, calculate\n",
        "        full cost matrix and return outputs of sinkhorn function.\n",
        "    batchSize: int or tuple of 2 int, optional\n",
        "        Size of the batches used to compute the sinkhorn update without memory overhead.\n",
        "        When a tuple is provided it sets the size of the left/right batches.\n",
        "    verbose : bool, optional\n",
        "        Print information along iterations\n",
        "    log : bool, optional\n",
        "        record log if True\n",
        "    warn : bool, optional\n",
        "        if True, raises a warning if the algorithm doesn't convergence.\n",
        "\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    W : (n_hists) array-like or float\n",
        "        Optimal transportation loss for the given parameters\n",
        "    log : dict\n",
        "        log dictionary return only if log==True in parameters\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "\n",
        "    >>> n_samples_a = 2\n",
        "    >>> n_samples_b = 2\n",
        "    >>> reg = 0.1\n",
        "    >>> X_s = np.reshape(np.arange(n_samples_a, dtype=np.float64), (n_samples_a, 1))\n",
        "    >>> X_t = np.reshape(np.arange(0, n_samples_b, dtype=np.float64), (n_samples_b, 1))\n",
        "    >>> b = np.full((n_samples_b, 3), 1/n_samples_b)\n",
        "    >>> empirical_sinkhorn2(X_s, X_t, b=b, reg=reg, verbose=False)\n",
        "    array([4.53978687e-05, 4.53978687e-05, 4.53978687e-05])\n",
        "\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "\n",
        "    .. [2] M. Cuturi, Sinkhorn Distances : Lightspeed Computation\n",
        "        of Optimal Transport, Advances in Neural Information\n",
        "        Processing Systems (NIPS) 26, 2013\n",
        "\n",
        "    .. [9] Schmitzer, B. (2016). Stabilized Sparse Scaling\n",
        "        Algorithms for Entropy Regularized Transport Problems.\n",
        "        arXiv preprint arXiv:1610.06519.  [Titel anhand dieser ArXiv-ID in Citavi-Projekt übernehmen] \n",
        "\n",
        "    .. [10] Chizat, L., Peyré, G., Schmitzer, B., & Vialard, F. X. (2016).\n",
        "        Scaling algorithms for unbalanced transport problems.\n",
        "        arXiv preprint arXiv:1607.05816.  [Titel anhand dieser ArXiv-ID in Citavi-Projekt übernehmen] \n",
        "    '''\n",
        "\n",
        "    X_s, X_t = ot.utils.list_to_array(X_s, X_t)\n",
        "\n",
        "    nx = ot.backend.get_backend(X_s, X_t)\n",
        "\n",
        "    ns, nt = X_s.shape[0], X_t.shape[0]\n",
        "    if a is None:\n",
        "        a = nx.from_numpy(unif(ns), type_as=X_s)\n",
        "    if b is None:\n",
        "        b = nx.from_numpy(unif(nt), type_as=X_s)\n",
        "\n",
        "    if isLazy:\n",
        "        if log:\n",
        "            f, g, dict_log = ot.bregman.empirical_sinkhorn(X_s, X_t, reg, a, b, metric,\n",
        "                                                numIterMax=numIterMax,\n",
        "                                                stopThr=stopThr,\n",
        "                                                isLazy=isLazy,\n",
        "                                                batchSize=batchSize,\n",
        "                                                verbose=verbose, log=log,\n",
        "                                                warn=warn)\n",
        "        else:\n",
        "            f, g = ot.bregman.empirical_sinkhorn(X_s, X_t, reg, a, b, metric,\n",
        "                                      numIterMax=numIterMax, stopThr=stopThr,\n",
        "                                      isLazy=isLazy, batchSize=batchSize,\n",
        "                                      verbose=verbose, log=log,\n",
        "                                      warn=warn)\n",
        "\n",
        "        bs = batchSize if isinstance(batchSize, int) else batchSize[0]\n",
        "        range_s = range(0, ns, bs)\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        X_s_np = nx.to_numpy(X_s)\n",
        "        X_t_np = nx.to_numpy(X_t)\n",
        "\n",
        "        for i in range_s:\n",
        "            M_block = dist_adj(X_s_np[i:i + bs, :], X_t_np, metric=metric)\n",
        "            M_block = nx.from_numpy(M_block, type_as=a)\n",
        "            pi_block = nx.exp(f[i:i + bs, None] + g[None, :] - M_block / reg)\n",
        "            loss += nx.sum(M_block * pi_block)\n",
        "\n",
        "        if log:\n",
        "            return loss, dict_log\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "    else:\n",
        "        M = dist_adj(nx.to_numpy(X_s), nx.to_numpy(X_t), metric=metric)\n",
        "        M = nx.from_numpy(M, type_as=a)\n",
        "\n",
        "        if log:\n",
        "            sinkhorn_loss, log = ot.bregman.sinkhorn2(a, b, M, reg, numItermax=numIterMax,\n",
        "                                           stopThr=stopThr, verbose=verbose, log=log,\n",
        "                                           warn=warn, **kwargs)\n",
        "            return sinkhorn_loss, log\n",
        "        else:\n",
        "            sinkhorn_loss = ot.bregman.sinkhorn2(a, b, M, reg, numItermax=numIterMax,\n",
        "                                      stopThr=stopThr, verbose=verbose, log=log,\n",
        "                                      warn=warn, **kwargs)\n",
        "            return sinkhorn_loss"
      ],
      "metadata": {
        "id": "Sc9A4qepriZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dist_adj(x1, x2=None, metric='sqeuclidean'):\n",
        "    r\"\"\"Compute distance between samples in :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`\n",
        "\n",
        "    .. note:: This function is backend-compatible and will work on arrays\n",
        "        from all compatible backends.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    x1 : array-like, shape (n1,d)\n",
        "        matrix with `n1` samples of size `d`\n",
        "    x2 : array-like, shape (n2,d), optional\n",
        "        matrix with `n2` samples of size `d` (if None then :math:`\\mathbf{x_2} = \\mathbf{x_1}`)\n",
        "    metric : str | callable, optional\n",
        "        'sqeuclidean' or 'euclidean' on all backends. On numpy the function also\n",
        "        accepts  from the scipy.spatial.distance.cdist function : 'braycurtis',\n",
        "        'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice',\n",
        "        'euclidean', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',\n",
        "        'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n",
        "        'sokalmichener', 'sokalsneath', 'sqeuclidean', 'wminkowski', 'yule'.\n",
        "\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    M : array-like, shape (`n1`, `n2`)\n",
        "        distance matrix computed with given metric\n",
        "\n",
        "    \"\"\"\n",
        "    if x2 is None:\n",
        "        x2 = x1\n",
        "    #if metric == \"sqeuclidean\":\n",
        "    #    return euclidean_distances(x1, x2, squared=True)\n",
        "    #elif metric == \"euclidean\":\n",
        "    #    return euclidean_distances(x1, x2, squared=False)\n",
        "    #else:\n",
        "    if not ot.backend.get_backend(x1, x2).__name__ == 'numpy':\n",
        "        raise NotImplementedError()\n",
        "    else:\n",
        "        return distance.cdist(x1, x2, metric=metric)"
      ],
      "metadata": {
        "id": "nS-hZx7xQybR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}